{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd0181",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4f2e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_auc_score, roc_curve\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b3441f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n",
      "(60000,) (10000,)\n",
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "(60000, 10) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d083ef37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 [==============================] - 12s 24ms/step - loss: 2.3008 - accuracy: 0.1076 - val_loss: 2.2855 - val_accuracy: 0.1081\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 2.2849 - accuracy: 0.1266 - val_loss: 2.2702 - val_accuracy: 0.1567\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 2.2714 - accuracy: 0.1499 - val_loss: 2.2557 - val_accuracy: 0.2165\n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 2.2597 - accuracy: 0.1734 - val_loss: 2.2416 - val_accuracy: 0.3018\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 2.2459 - accuracy: 0.1928 - val_loss: 2.2266 - val_accuracy: 0.3871\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 2.2337 - accuracy: 0.2119 - val_loss: 2.2103 - val_accuracy: 0.4618\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 2.2191 - accuracy: 0.2324 - val_loss: 2.1924 - val_accuracy: 0.5094\n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 2.2027 - accuracy: 0.2516 - val_loss: 2.1724 - val_accuracy: 0.5388\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 2.1860 - accuracy: 0.2659 - val_loss: 2.1503 - val_accuracy: 0.5533\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 2.1655 - accuracy: 0.2854 - val_loss: 2.1251 - val_accuracy: 0.5658\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 2.1465 - accuracy: 0.2957 - val_loss: 2.0978 - val_accuracy: 0.5788\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 2.1246 - accuracy: 0.3096 - val_loss: 2.0674 - val_accuracy: 0.5869\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 2.0994 - accuracy: 0.3240 - val_loss: 2.0337 - val_accuracy: 0.5961\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 2.0721 - accuracy: 0.3365 - val_loss: 1.9970 - val_accuracy: 0.6035\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 2.0428 - accuracy: 0.3476 - val_loss: 1.9573 - val_accuracy: 0.6117\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 2.0154 - accuracy: 0.3587 - val_loss: 1.9161 - val_accuracy: 0.6189\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 1.9869 - accuracy: 0.3691 - val_loss: 1.8733 - val_accuracy: 0.6270\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.9534 - accuracy: 0.3821 - val_loss: 1.8283 - val_accuracy: 0.6325\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.9195 - accuracy: 0.3925 - val_loss: 1.7820 - val_accuracy: 0.6398\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 1.8859 - accuracy: 0.4038 - val_loss: 1.7353 - val_accuracy: 0.6460\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 1.8553 - accuracy: 0.4090 - val_loss: 1.6892 - val_accuracy: 0.6524\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.8175 - accuracy: 0.4226 - val_loss: 1.6406 - val_accuracy: 0.6604\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.7861 - accuracy: 0.4306 - val_loss: 1.5937 - val_accuracy: 0.6674\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.7528 - accuracy: 0.4405 - val_loss: 1.5474 - val_accuracy: 0.6739\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.7219 - accuracy: 0.4477 - val_loss: 1.5025 - val_accuracy: 0.6827\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 1.6905 - accuracy: 0.4569 - val_loss: 1.4586 - val_accuracy: 0.6888\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 1.6569 - accuracy: 0.4668 - val_loss: 1.4152 - val_accuracy: 0.6952\n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 12s 25ms/step - loss: 1.6284 - accuracy: 0.4737 - val_loss: 1.3741 - val_accuracy: 0.7024\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 13s 28ms/step - loss: 1.5990 - accuracy: 0.4836 - val_loss: 1.3336 - val_accuracy: 0.7134\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 12s 26ms/step - loss: 1.5692 - accuracy: 0.4896 - val_loss: 1.2949 - val_accuracy: 0.7178\n",
      "The model has successfully trained\n",
      "Test loss: 1.2948583364486694\n",
      "Test accuracy: 0.7178000211715698\n",
      "Saving the model as mnist.h5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 30\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "hist = model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))\n",
    "print(\"The model has successfully trained\")\n",
    "y_pred = model.predict(x_test)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "model.save('mnist.h5')\n",
    "print(\"Saving the model as mnist.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe971d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.32941177]\n",
      "  [0.7254902 ]\n",
      "  [0.62352943]\n",
      "  [0.5921569 ]\n",
      "  [0.23529412]\n",
      "  [0.14117648]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.87058824]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.94509804]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.7764706 ]\n",
      "  [0.6666667 ]\n",
      "  [0.20392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.2627451 ]\n",
      "  [0.44705883]\n",
      "  [0.28235295]\n",
      "  [0.44705883]\n",
      "  [0.6392157 ]\n",
      "  [0.8901961 ]\n",
      "  [0.99607843]\n",
      "  [0.88235295]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.98039216]\n",
      "  [0.8980392 ]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.54901963]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.06666667]\n",
      "  [0.25882354]\n",
      "  [0.05490196]\n",
      "  [0.2627451 ]\n",
      "  [0.2627451 ]\n",
      "  [0.2627451 ]\n",
      "  [0.23137255]\n",
      "  [0.08235294]\n",
      "  [0.9254902 ]\n",
      "  [0.99607843]\n",
      "  [0.41568628]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.3254902 ]\n",
      "  [0.99215686]\n",
      "  [0.81960785]\n",
      "  [0.07058824]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.08627451]\n",
      "  [0.9137255 ]\n",
      "  [1.        ]\n",
      "  [0.3254902 ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.5058824 ]\n",
      "  [0.99607843]\n",
      "  [0.93333334]\n",
      "  [0.17254902]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.23137255]\n",
      "  [0.9764706 ]\n",
      "  [0.99607843]\n",
      "  [0.24313726]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.52156866]\n",
      "  [0.99607843]\n",
      "  [0.73333335]\n",
      "  [0.01960784]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.03529412]\n",
      "  [0.8039216 ]\n",
      "  [0.972549  ]\n",
      "  [0.22745098]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.49411765]\n",
      "  [0.99607843]\n",
      "  [0.7137255 ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.29411766]\n",
      "  [0.9843137 ]\n",
      "  [0.9411765 ]\n",
      "  [0.22352941]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.07450981]\n",
      "  [0.8666667 ]\n",
      "  [0.99607843]\n",
      "  [0.6509804 ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.01176471]\n",
      "  [0.79607844]\n",
      "  [0.99607843]\n",
      "  [0.85882354]\n",
      "  [0.13725491]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.14901961]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.3019608 ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.12156863]\n",
      "  [0.8784314 ]\n",
      "  [0.99607843]\n",
      "  [0.4509804 ]\n",
      "  [0.00392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.52156866]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.20392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.23921569]\n",
      "  [0.9490196 ]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.20392157]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.4745098 ]\n",
      "  [0.99607843]\n",
      "  [0.99607843]\n",
      "  [0.85882354]\n",
      "  [0.15686275]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.4745098 ]\n",
      "  [0.99607843]\n",
      "  [0.8117647 ]\n",
      "  [0.07058824]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]\n",
      "\n",
      " [[0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]\n",
      "  [0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "model = load_model('mnist.h5')\n",
    "def predict_digit(img):\n",
    "    #resize image to 28x28 pixels\n",
    "    img = img.resize((28,28))\n",
    "    #convert rgb to grayscale\n",
    "    img = img.convert('L')\n",
    "    img = np.array(img)\n",
    "    img = img.reshape(1,28,28,1)\n",
    "    img = img/255.0\n",
    "    img = 1 - img\n",
    "    #predicting\n",
    "    res = model.predict([img])[0]\n",
    "    return np.argmax(res), max(res)\n",
    "\n",
    "#predict_digit(x_test[0])\n",
    "#print(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ae46f8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ytest [[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (10000, 10)\n",
      "[[0.04856478 0.0194943  0.03290245 ... 0.52600086 0.07617689 0.11400586]\n",
      " [0.08071245 0.15030333 0.13693435 ... 0.01903542 0.05838073 0.02619325]\n",
      " [0.0270273  0.4376053  0.04995548 ... 0.052628   0.06814744 0.08540028]\n",
      " ...\n",
      " [0.02561745 0.04937463 0.03933642 ... 0.20489398 0.14777182 0.25342506]\n",
      " [0.0579846  0.11702707 0.04701737 ... 0.08641621 0.17398855 0.09249549]\n",
      " [0.14104049 0.02117074 0.11381148 ... 0.00651915 0.02805488 0.01935799]]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 943,    1,    2,   11,    0,    0,   15,    1,    7,    0],\n",
       "       [   0, 1123,    0,    6,    0,    1,    5,    0,    0,    0],\n",
       "       [  62,   41,  685,   35,    1,    0,  143,   48,   14,    3],\n",
       "       [  14,   24,   10,  889,    0,    0,   14,   23,   25,   11],\n",
       "       [  16,   17,   20,    0,  212,    0,   58,   56,   17,  586],\n",
       "       [  79,   86,    0,  300,    9,  258,   66,   40,   35,   19],\n",
       "       [  37,   40,    5,    0,    0,    2,  871,    1,    1,    1],\n",
       "       [   7,   31,   22,    1,    1,    0,    5,  877,    5,   79],\n",
       "       [  54,  128,   16,  105,    1,    6,   26,   70,  498,   70],\n",
       "       [  30,   27,   11,   13,   10,    0,    4,   83,    9,  822]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "print(\"ytest\",y_test,y_test.shape)\n",
    "print(y_pred)\n",
    "yp = np.round([[1 if e == np.argmax(arr) else 0 for arr in y_pred for e in range(len(arr))]]).reshape(y_test.shape[0],-1).astype(\"float32\")\n",
    "yp.astype(\"float32\")\n",
    "yp\n",
    "print(type(y_test), type(yp))\n",
    "\n",
    "confusion_matrix(\n",
    "    y_test.argmax(axis=1), yp.argmax(axis=1))\n",
    "\n",
    "#array([[1, 0],\n",
    "#       [0, 2]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e5877878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[ 943    1    2   11    0    0   15    1    7    0]\n",
      " [   0 1123    0    6    0    1    5    0    0    0]\n",
      " [  62   41  685   35    1    0  143   48   14    3]\n",
      " [  14   24   10  889    0    0   14   23   25   11]\n",
      " [  16   17   20    0  212    0   58   56   17  586]\n",
      " [  79   86    0  300    9  258   66   40   35   19]\n",
      " [  37   40    5    0    0    2  871    1    1    1]\n",
      " [   7   31   22    1    1    0    5  877    5   79]\n",
      " [  54  128   16  105    1    6   26   70  498   70]\n",
      " [  30   27   11   13   10    0    4   83    9  822]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[7 6 1 ... 9 5 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7178\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.96      0.85       980\n",
      "           1       0.74      0.99      0.85      1135\n",
      "           2       0.89      0.66      0.76      1032\n",
      "           3       0.65      0.88      0.75      1010\n",
      "           4       0.91      0.22      0.35       982\n",
      "           5       0.97      0.29      0.45       892\n",
      "           6       0.72      0.91      0.80       958\n",
      "           7       0.73      0.85      0.79      1028\n",
      "           8       0.82      0.51      0.63       974\n",
      "           9       0.52      0.81      0.63      1009\n",
      "\n",
      "   micro avg       0.72      0.72      0.72     10000\n",
      "   macro avg       0.77      0.71      0.69     10000\n",
      "weighted avg       0.77      0.72      0.69     10000\n",
      " samples avg       0.72      0.72      0.72     10000\n",
      "\n",
      "Precision score: 0.76708538232832\n",
      "Recall: 0.7178\n",
      "F1 score 0.69046327321788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8387400186625881"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cmap=plt.cm.Blues\n",
    "print(\"\\nConfusion matrix:\\n\",metrics.confusion_matrix(y_test.argmax(axis=1), yp.argmax(axis=1)) )\n",
    "cm = confusion_matrix(y_test.argmax(axis=1), yp.argmax(axis=1))\n",
    "print(y_test.argmax(axis=1))\n",
    "print(yp.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, yp))\n",
    "print(\"Classification report:\\n\",metrics.classification_report(y_test, yp))\n",
    "print(\"Precision score:\",metrics.precision_score(y_test, yp, average= 'weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, yp, average= 'weighted'))\n",
    "print(\"F1 score\",metrics.f1_score(y_test, yp, average= 'weighted'))\n",
    "\n",
    "roc_auc_score(y_test,yp,multi_class='ova')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "843b1a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 6 4 4 6 4 4 3 7 2 3 6 3 5 1 2 5 5 3 5]\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d49cc88",
   "metadata": {},
   "source": [
    "# TYPE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6ee95403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27999, 785)\n",
      "(10000, 785)\n",
      "x_train shape: (27999, 28, 28, 1)\n",
      "27999 train samples\n",
      "10000 test samples\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = pd.read_csv('mnist_train.csv')\n",
    "df_test = pd.read_csv('mnist_test.csv')\n",
    "\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "X_train = df_train.iloc[:,1:].values\n",
    "X_test = df_test.iloc[:,1:].values\n",
    "ytrain = df_train.iloc[:,0].values\n",
    "ytest = df_test.iloc[:,0].values\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "\n",
    "ytrain = keras.utils.to_categorical(ytrain, 10)\n",
    "ytest = keras.utils.to_categorical(ytest, 10)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('x_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "print(ytrain)\n",
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "195f8418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "219/219 [==============================] - 7s 29ms/step - loss: 2.3087 - accuracy: 0.1090 - val_loss: 2.2945 - val_accuracy: 0.1079\n",
      "Epoch 2/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.2984 - accuracy: 0.1188 - val_loss: 2.2833 - val_accuracy: 0.1329\n",
      "Epoch 3/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.2877 - accuracy: 0.1284 - val_loss: 2.2726 - val_accuracy: 0.1765\n",
      "Epoch 4/30\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 2.2789 - accuracy: 0.1409 - val_loss: 2.2624 - val_accuracy: 0.2335\n",
      "Epoch 5/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.2697 - accuracy: 0.1554 - val_loss: 2.2528 - val_accuracy: 0.3029\n",
      "Epoch 6/30\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 2.2616 - accuracy: 0.1636 - val_loss: 2.2433 - val_accuracy: 0.3599\n",
      "Epoch 7/30\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 2.2536 - accuracy: 0.1752 - val_loss: 2.2336 - val_accuracy: 0.4010\n",
      "Epoch 8/30\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 2.2452 - accuracy: 0.1860 - val_loss: 2.2233 - val_accuracy: 0.4285\n",
      "Epoch 9/30\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 2.2360 - accuracy: 0.1949 - val_loss: 2.2123 - val_accuracy: 0.4464\n",
      "Epoch 10/30\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 2.2272 - accuracy: 0.2079 - val_loss: 2.2010 - val_accuracy: 0.4589\n",
      "Epoch 11/30\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 2.2166 - accuracy: 0.2221 - val_loss: 2.1888 - val_accuracy: 0.4708\n",
      "Epoch 12/30\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 2.2085 - accuracy: 0.2262 - val_loss: 2.1764 - val_accuracy: 0.4812\n",
      "Epoch 13/30\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 2.1956 - accuracy: 0.2413 - val_loss: 2.1632 - val_accuracy: 0.4934\n",
      "Epoch 14/30\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 2.1855 - accuracy: 0.2501 - val_loss: 2.1496 - val_accuracy: 0.5097\n",
      "Epoch 15/30\n",
      "219/219 [==============================] - 7s 32ms/step - loss: 2.1754 - accuracy: 0.2544 - val_loss: 2.1353 - val_accuracy: 0.5302\n",
      "Epoch 16/30\n",
      "219/219 [==============================] - 6s 30ms/step - loss: 2.1639 - accuracy: 0.2666 - val_loss: 2.1203 - val_accuracy: 0.5460\n",
      "Epoch 17/30\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 2.1544 - accuracy: 0.2689 - val_loss: 2.1049 - val_accuracy: 0.5598\n",
      "Epoch 18/30\n",
      "219/219 [==============================] - 7s 34ms/step - loss: 2.1404 - accuracy: 0.2839 - val_loss: 2.0888 - val_accuracy: 0.5713\n",
      "Epoch 19/30\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 2.1266 - accuracy: 0.2892 - val_loss: 2.0717 - val_accuracy: 0.5851\n",
      "Epoch 20/30\n",
      "219/219 [==============================] - 8s 37ms/step - loss: 2.1145 - accuracy: 0.3011 - val_loss: 2.0542 - val_accuracy: 0.5953\n",
      "Epoch 21/30\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 2.1020 - accuracy: 0.3072 - val_loss: 2.0359 - val_accuracy: 0.6120\n",
      "Epoch 22/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.0861 - accuracy: 0.3125 - val_loss: 2.0167 - val_accuracy: 0.6238\n",
      "Epoch 23/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.0747 - accuracy: 0.3224 - val_loss: 1.9975 - val_accuracy: 0.6347\n",
      "Epoch 24/30\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 2.0595 - accuracy: 0.3298 - val_loss: 1.9773 - val_accuracy: 0.6433\n",
      "Epoch 25/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.0414 - accuracy: 0.3350 - val_loss: 1.9565 - val_accuracy: 0.6575\n",
      "Epoch 26/30\n",
      "219/219 [==============================] - 7s 31ms/step - loss: 2.0294 - accuracy: 0.3411 - val_loss: 1.9352 - val_accuracy: 0.6714\n",
      "Epoch 27/30\n",
      "219/219 [==============================] - 6s 29ms/step - loss: 2.0130 - accuracy: 0.3497 - val_loss: 1.9134 - val_accuracy: 0.6813\n",
      "Epoch 28/30\n",
      "219/219 [==============================] - 8s 36ms/step - loss: 1.9963 - accuracy: 0.3603 - val_loss: 1.8908 - val_accuracy: 0.6906\n",
      "Epoch 29/30\n",
      "219/219 [==============================] - 7s 33ms/step - loss: 1.9790 - accuracy: 0.3624 - val_loss: 1.8676 - val_accuracy: 0.7019\n",
      "Epoch 30/30\n",
      "219/219 [==============================] - 7s 30ms/step - loss: 1.9648 - accuracy: 0.3702 - val_loss: 1.8443 - val_accuracy: 0.7092\n",
      "The model has successfully trained\n",
      "Test loss: 1.8443269729614258\n",
      "Test accuracy: 0.7092000246047974\n",
      "Saving the model as Mnist.h5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 30\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),activation='relu',input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(),metrics=['accuracy'])\n",
    "hist = model.fit(X_train, ytrain,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(X_test, ytest))\n",
    "print(\"The model has successfully trained\")\n",
    "ypred = model.predict(X_test)\n",
    "score = model.evaluate(X_test, ytest, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "model.save('Mnist.h5')\n",
    "print(\"Saving the model as Mnist.h5\")\n",
    "model = load_model('Mnist.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1b9170ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ytest [[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]] (10000, 10)\n",
      "[[0.08 0.09 0.09 ... 0.21 0.08 0.1 ]\n",
      " [0.11 0.09 0.14 ... 0.07 0.12 0.04]\n",
      " [0.08 0.14 0.11 ... 0.1  0.13 0.07]\n",
      " ...\n",
      " [0.06 0.1  0.07 ... 0.14 0.12 0.12]\n",
      " [0.09 0.11 0.09 ... 0.1  0.13 0.07]\n",
      " [0.15 0.07 0.12 ... 0.07 0.07 0.06]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "print(\"ytest\",ytest,ytest.shape)\n",
    "print(ypred)\n",
    "y_p = np.round([[1 if e == np.argmax(arr) else 0 for arr in ypred for e in range(len(arr))]]).reshape(ytest.shape[0],-1).astype(\"float32\")\n",
    "y_p.astype(\"float32\")\n",
    "y_p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3572d19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      " [[939   0   0   7   0   0  22   2  10   0]\n",
      " [  0 906   0  26   0   1  11  14 177   0]\n",
      " [ 66   1 728 102   5   1  58  33  38   0]\n",
      " [ 12   0  22 919   0   5   2  25  24   1]\n",
      " [ 10   1   0   7 317   0 204  53  31 359]\n",
      " [ 36   2   5 304  10 284  40  15 193   3]\n",
      " [ 43   1   5   0   1  18 860   0  30   0]\n",
      " [  4  16  24   2   4   0   4 937  25  12]\n",
      " [ 15   2   6 181   0  16  54  64 634   2]\n",
      " [ 24   2   0  32  50   0  32 268  33 568]]\n",
      "[7 2 1 ... 4 5 6]\n",
      "[7 3 1 ... 7 8 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7092\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88       980\n",
      "           1       0.97      0.80      0.88      1135\n",
      "           2       0.92      0.71      0.80      1032\n",
      "           3       0.58      0.91      0.71      1010\n",
      "           4       0.82      0.32      0.46       982\n",
      "           5       0.87      0.32      0.47       892\n",
      "           6       0.67      0.90      0.77       958\n",
      "           7       0.66      0.91      0.77      1028\n",
      "           8       0.53      0.65      0.58       974\n",
      "           9       0.60      0.56      0.58      1009\n",
      "\n",
      "   micro avg       0.71      0.71      0.71     10000\n",
      "   macro avg       0.75      0.70      0.69     10000\n",
      "weighted avg       0.75      0.71      0.70     10000\n",
      " samples avg       0.71      0.71      0.71     10000\n",
      "\n",
      "Precision score: 0.7473762255962716\n",
      "Recall: 0.7092\n",
      "F1 score 0.6952291766386669\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8356509869881412"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cmap=plt.cm.Blues\n",
    "print(\"\\nConfusion matrix:\\n\",metrics.confusion_matrix(ytest.argmax(axis=1), y_p.argmax(axis=1)) )\n",
    "cm = confusion_matrix(ytest.argmax(axis=1), y_p.argmax(axis=1))\n",
    "print(ytest.argmax(axis=1))\n",
    "print(y_p.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(ytest, y_p))\n",
    "print(\"Classification report:\\n\",metrics.classification_report(ytest, y_p))\n",
    "print(\"Precision score:\",metrics.precision_score(ytest, y_p, average= 'weighted'))\n",
    "print(\"Recall:\",metrics.recall_score(ytest, y_p, average= 'weighted'))\n",
    "print(\"F1 score\",metrics.f1_score(ytest, y_p, average= 'weighted'))\n",
    "\n",
    "roc_auc_score(ytest,y_p,multi_class='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed5559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
